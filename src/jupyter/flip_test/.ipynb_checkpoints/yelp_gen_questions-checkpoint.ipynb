{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.onnx\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import sys\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# my modules\n",
    "sys.path.append(os.getcwd() + '/../..')\n",
    "\n",
    "from models import ShallowCNN\n",
    "from models import DPCNN\n",
    "from dataset import dataset\n",
    "from preprocess import preprocess\n",
    "from util import readConfig\n",
    "from train import *\n",
    "from plot import plots\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr._core import (\n",
    "    input_x_gradient,\n",
    "    guided_grad_cam,\n",
    "    gradient_shap\n",
    ")\n",
    "\n",
    "\n",
    "from captum.attr import (\n",
    "    GradientShap,\n",
    "    DeepLift,\n",
    "    DeepLiftShap,\n",
    "    IntegratedGradients,\n",
    "    LayerConductance,\n",
    "    NeuronConductance,\n",
    "    NoiseTunnel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the config get the model and the data\n",
    "def init(config_path):\n",
    "    print('Read config file %s.'%config_path)\n",
    "    args = readConfig.readConfig(config_path)\n",
    "    print(args)\n",
    "    # load embeddings\n",
    "    embed_matrix = pickle.load(open(args['embed_path']+'/embed.pkl', 'rb'))\n",
    "    print(\"Load embedding of size %s\"%embed_matrix.shape[1])\n",
    "    w2i = pickle.load(open(args['w2i']+'/w2i.pkl', 'rb'))\n",
    "    i2w = pickle.load(open(args['i2w']+'/i2w.pkl', 'rb'))\n",
    "    # load data\n",
    "    cls, texts =preprocess.readData(args['input_path']+\"/test_classes.data\", args['input_path']+\"/test_texts.data\") \n",
    "    test_set = dataset.TextDataset(texts, cls, w2i, int(args['max_sen_len']))\n",
    "    model = None\n",
    "    \n",
    "    model_args = {\n",
    "        'vocab_size': embed_matrix.shape[0], # add unkown word\n",
    "        'max_len': int(args['max_sen_len']),\n",
    "        'n_class': int(args['num_class']),\n",
    "        'dim': embed_matrix.shape[1],\n",
    "        'dropout': float(args['dropout']),\n",
    "        'freeze': True,\n",
    "        'kernel_num': int(args['channel_size']),\n",
    "        'embedding_matrix': embed_matrix,\n",
    "        'forward_embed': True\n",
    "        \n",
    "    }\n",
    "    if args['model']=='shallowCNN':\n",
    "            print(\"Start training shallowCNN\")\n",
    "            model = ShallowCNN.ShallowCNN(model_args)\n",
    "    else:\n",
    "        model = DPCNN.DPCNN(model_args)\n",
    "    model.load(args['load_model_from'])\n",
    "    print(model.eval)\n",
    "    model.setDropout(0)\n",
    "    \n",
    "    return (args, w2i, i2w, cls, texts, test_set, model_args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read config file ../../train/abspath_shallowcnn_config.\n",
      "{'action': 'test', 'model': 'shallowCNN', 'max_sen_len': '129', 'num_class': '2', 'epoch': '4', 'cv': '0', 'dropout': '0.5', 'freeze': 'True', 'output_path': '/home/sj/Documents/lecture_projects/master_thesis_2020/explain_cnn_text_classifiers/data/models/yelp_polarity_DPCNN/', 'input_path': '/home/sj/Documents/lecture_projects/master_thesis_2020/explain_cnn_text_classifiers/data/datasets/yelp_polarity', 'channel_size': '20', 'embed_path': '/home/sj/Documents/lecture_projects/master_thesis_2020/explain_cnn_text_classifiers/data/embeddings/polar/selected1275_yelp', 'w2i': '/home/sj/Documents/lecture_projects/master_thesis_2020/explain_cnn_text_classifiers/data/embeddings/glove/yelp_polarity', 'i2w': '/home/sj/Documents/lecture_projects/master_thesis_2020/explain_cnn_text_classifiers/data/embeddings/glove/yelp_polarity', 'load_model_from': '/home/sj/Documents/lecture_projects/master_thesis_2020/explain_cnn_text_classifiers/data/models/yelp_polarity_shallowCNN/new_polar1275-3-139.model', 'load_dim_names': '/home/sj/Documents/lecture_projects/master_thesis_2020/explain_cnn_text_classifiers/data/embeddings/polar/selected1275_yelp/b1275.pkl', 'gradients': 'False', 'forward_embed': 'True'}\n",
      "Load embedding of size 1275\n",
      "Start training shallowCNN\n",
      "<bound method Module.eval of ShallowCNN(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (embedding_layer): Embedding(54928, 1275)\n",
      "  (conv11): Conv2d(1, 20, kernel_size=(3, 1275), stride=(1, 1))\n",
      "  (conv12): Conv2d(1, 20, kernel_size=(3, 1275), stride=(1, 1))\n",
      "  (conv13): Conv2d(1, 20, kernel_size=(4, 1275), stride=(1, 1))\n",
      "  (conv14): Conv2d(1, 20, kernel_size=(4, 1275), stride=(1, 1))\n",
      "  (conv15): Conv2d(1, 20, kernel_size=(5, 1275), stride=(1, 1))\n",
      "  (conv16): Conv2d(1, 20, kernel_size=(5, 1275), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=120, out_features=2, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "args, w2i, i2w, cls, texts, test_set, model_args, model = init(\"../../train/abspath_shallowcnn_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads(args, w2i, i2w, cls, texts, test_set, model_args, model, \n",
    "                  input_index=0, label=0, \n",
    "                  method=\"Integrated-Gradient\", \n",
    "                  topKWords=10, \n",
    "                  topKDims=5, \n",
    "                  embeddings=''):\n",
    "        \n",
    "    p = input_index\n",
    "    model.forwardEmbedLayer = False\n",
    "    num_rows = min(129, len(texts[p]))\n",
    "\n",
    "    input = test_set[p][0]\n",
    "    label = label#test_set[p][1]\n",
    "    input = embeddings(input) if embeddings else model.embedding_layer(input)\n",
    "    input = input.view(1, 1, model_args['max_len'], model_args['dim'])\n",
    "    out = model(input)\n",
    "    predicted = torch.argmax(out, dim=1).tolist()[0]\n",
    "    output_scores = (out[0].detach().numpy())\n",
    "    attributions_out = None\n",
    "    # methods return grads\n",
    "    if method==\"Integrated-Gradient\":\n",
    "        ig = IntegratedGradients(model)\n",
    "        baseline = torch.zeros((model_args['max_len'],model_args['dim']))\n",
    "        baseline = baseline.view(1, 1, model_args['max_len'], model_args['dim'])\n",
    "        attributions_out, delta = ig.attribute(input, baseline, target=label, return_convergence_delta=True)\n",
    "    elif method==\"InputXGrad\":\n",
    "        attributions_out = DeepLift(model).attribute(input, target=label)\n",
    "#         attributions_out = input_x_gradient.InputXGradient(model).attribute(input, target=label)\n",
    "    elif method==\"Grad-Cam\":\n",
    "        attributions_out = guided_grad_cam.GuidedGradCam(model, model.conv3).attribute(input, target=label)\n",
    "    elif method==\"SHAP\":\n",
    "        shap = gradient_shap.GradientShap(model)\n",
    "        baseline = torch.zeros((model_args['max_len'],model_args['dim']))\n",
    "        baseline = baseline.view(1, 1, model_args['max_len'], model_args['dim'])\n",
    "        num_samples = 500\n",
    "        attributions_out, delta = shap.attribute(input, baseline, n_samples=num_samples, target=label, return_convergence_delta=True)\n",
    "    \n",
    "    else:\n",
    "#         print(\"Skip computing grad, return outputs.\")\n",
    "#         print(input.view(model_args['max_len'], model_args['dim']).detach().numpy())\n",
    "        return output_scores\n",
    "\n",
    "    attributions = attributions_out.view(model_args['max_len'], model_args['dim']).detach().numpy()\n",
    "    _input = input.view(model_args['max_len'], model_args['dim']).detach().numpy()\n",
    "#     attributions = (mat_elem_devide(attributions, _input))\n",
    "\n",
    "    # flaten\n",
    "    flat_attribution = attributions.reshape((model_args['max_len'] * model_args['dim'], ))\n",
    "    flat_attribution = stats.zscore(flat_attribution, ddof=1)\n",
    "\n",
    "    # check top K contributing words\n",
    "    K = min(topKWords,len(texts[p]))\n",
    "    num_top_dims = topKDims\n",
    "    # sort by sum\n",
    "    word_contributions = [(i, np.sum(attributions[i])) for i in range(num_rows) ]\n",
    "    word_contributions = sorted(word_contributions, key=lambda x: x[1], reverse=True)\n",
    "    topk_words = [i for i,v in word_contributions[:K]]\n",
    "    dict_ranks = {}\n",
    "    for i in range(K):\n",
    "        dict_ranks[topk_words[i]] = i\n",
    "        \n",
    "    # check over all top contribution dims\n",
    "    dim_contributions = [(i, np.sum(column)) for i, column in enumerate(attributions.T)]\n",
    "    dim_contributions = sorted(dim_contributions, key=lambda x:x[1], reverse=True)\n",
    "        \n",
    "\n",
    "\n",
    "    # load dim names\n",
    "    antonym_names = [b for a,b in pickle.load(open(args['load_dim_names'], 'rb'))]\n",
    "    # build sorted list of attributions\n",
    "    flat_attr_sorted = []\n",
    "    # remember position in flat attribution list\n",
    "    pos_counter = 0 \n",
    "    word_counter = {}\n",
    "            \n",
    "    for i in range(num_rows):\n",
    "        w = texts[p][i]\n",
    "        word_counter[w] = word_counter.get(w, 0) + 1\n",
    "        # print(pos_counter / model_args['dim'])\n",
    "        row = flat_attribution[pos_counter: pos_counter+model_args['dim']]\n",
    "        temp_list = []\n",
    "        if i in topk_words:\n",
    "            for t in range(model_args['dim']):\n",
    "                attr_value = row[t]\n",
    "                a1 = antonym_names[t][0]\n",
    "                a2 = antonym_names[t][1]\n",
    "                w_contrib = dict(word_contributions)[i]\n",
    "                word_pair = (a1,a2) #if attr_value < 0 else (a2,a1)\n",
    "                temp_list.append( (w, word_counter[w], word_pair, abs(attr_value), dict_ranks[i], w_contrib))\n",
    "\n",
    "#             temp_list = sorted(temp_list, key=lambda x: x[3], reverse=True)\n",
    "            \n",
    "            flat_attr_sorted.append(temp_list)\n",
    "        pos_counter += model_args['dim']\n",
    "            \n",
    "    # construct dict as output\n",
    "    dict_to_json = {\n",
    "                'Text' : text_sentences[p],\n",
    "                'Prediction' : 'Positive' if predicted > 0 else 'Negative',\n",
    "                'True-Label' : 'Positive' if test_set[p][1] > 0 else 'Negative',\n",
    "                'Out' : [round(output_scores[0],6), round(output_scores[1], 6)],\n",
    "                'Top-Dims':[(antonym_names[i], v) for i,v in dim_contributions]\n",
    "                \n",
    "            }\n",
    "    \n",
    "    for index in range(len(flat_attr_sorted)):\n",
    "        word_dict = {}\n",
    "        antonym_dict = {}\n",
    "        for w, wc, antonym_pair, v, rank, contrib in flat_attr_sorted[index]:\n",
    "            if not word_dict:\n",
    "                word_dict['Word'] = w \n",
    "                word_dict['Contribution'] = str(word_contributions[topk_words[index]][1])\n",
    "                word_dict[\"Rank\"] = rank\n",
    "                word_dict[\"Contribution\"] = contrib\n",
    "                # [sentence_index, word_index in the sentence]\n",
    "                pos = [0,0]\n",
    "                # here I want to find the position of the word in the original text,\n",
    "                # not the position in the preprocessed input text where stop words are removed, \n",
    "                # so it will be messy here\n",
    "                for sent_index in range(len(text_sentences[p])):\n",
    "                    pos[0] = sent_index\n",
    "                    words = text_sentences[p][sent_index].split(' ')\n",
    "                    for word_index in range(len(words)):\n",
    "                        pos[1] = word_index\n",
    "                        if words[word_index].lower() == w.lower():\n",
    "                            wc -= 1\n",
    "                            # print(word, type(wc), type(pos[1]))\n",
    "                            if wc == 0:\n",
    "                                # print(word)\n",
    "                                word_dict['Position'] = pos\n",
    "                                break\n",
    "                    if word_dict.get('Position'):\n",
    "                        break\n",
    "\n",
    "            antonym_dict[antonym_pair[0]+','+ antonym_pair[1]] = v\n",
    "            antonym_dict = dict(sorted(antonym_dict.items(), key=lambda x:x[1], reverse=True))\n",
    "        word_dict['Antonyms'] = antonym_dict\n",
    "        dict_to_json[word_dict['Word']+\"_\"+str(index)] = word_dict\n",
    "        \n",
    "    return dict_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
